{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the IMDB dataset\n",
    "\n",
    "vocab_size = 10000\n",
    "(X_train,y_train), (X_test,y_test) = imdb.load_data(num_words=vocab_size)\n",
    "print(f'X_train: {X_train.shape} - y_train: {y_train.shape}  X_test:{X_test.shape} - y_test:{y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Review X_train \n",
    "\n",
    "sample_review = X_train[0]\n",
    "sample_review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of word index back to words\n",
    "\n",
    "word_to_index = imdb.get_word_index() # Returns all the word to index dict of vocab\n",
    "\n",
    "index_to_word = {index+3:word for word,index in word_to_index.items()} #Reversing the word to index - index to word\n",
    "\n",
    "\n",
    "''' \n",
    "Why add + 3 to the index?\n",
    "\n",
    "imdb.get_word_index() does not account for the reserved indices for \n",
    "special tokens (<PAD>, <START>, <UNK>, <UNUSED>). \n",
    "\n",
    "When loading the dataset using imdb.load_data, the data is preprocessed to include reserved tokens:\n",
    "0 for <PAD>: Used for padding sequences to the same length.\n",
    "1 for <START>: Marks the beginning of a review.\n",
    "2 for <UNK>: Replaces words that are not in the top num_words most frequent words.\n",
    "3 for <UNUSED>: Reserved for future use.\n",
    "\n",
    "As a result, the indices in the reviews (e.g., X_train, X_test) \n",
    "start from 4, and the word indices need to align accordingly.\n",
    "\n",
    "'''\n",
    "# Decode X_train[item]-> word indices to words\n",
    "\n",
    "def decode_review(review_index=0):\n",
    "    \"\"\"\n",
    "    Decodes a review from the IMDB dataset using index_to_word mapping.\n",
    "\n",
    "    Args:\n",
    "        review_index (int): The index of the review in X_train to decode. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        str: The decoded review as a string of words.\n",
    "    \"\"\"\n",
    "    return \" \".join(index_to_word.get(index, '<UNK>') for index in X_train[review_index])\n",
    "\n",
    "# Example usage\n",
    "item = 0\n",
    "print(f\"X_train[{item}] decoded review: {decode_review(item)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding the each item in X_train and X_test to have max length\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train,maxlen=500)\n",
    "X_test = sequence.pad_sequences(X_test,maxlen=500)\n",
    "\n",
    "'''\n",
    "By default it takes 'pre' padding\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Simple RNN\n",
    "\n",
    "# Initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding layer: Converts integer indices into dense vectors of fixed size (128)\n",
    "model.add(Embedding(vocab_size, 128, input_length=500))\n",
    "\n",
    "# SimpleRNN layer: RNN with 128 neurons\n",
    "model.add(SimpleRNN(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "\n",
    "# Dropout Layer for Regularization\n",
    "model.add(Dropout(0.2)) # 20% Dropout rate\n",
    "\n",
    "# Dense output layer with a single neuron (for binary classification)\n",
    "model.add(Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.001)))\n",
    "\n",
    "# Build the model with the input shape\n",
    "model.build(input_shape=(None, 500))  # The input shape should match the shape of the training data\n",
    "\n",
    "optimizer = Adam(learning_rate = 1e-4)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss ='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up EarlyStopping\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss',patience=2,restore_best_weights=True)\n",
    "\n",
    "early_stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traing the Model with EarlyStopping\n",
    "\n",
    "model.fit(X_train,\n",
    "          y_train,\n",
    "          epochs=10,\n",
    "          batch_size=32,\n",
    "          validation_split =0.2,\n",
    "          callbacks=[early_stopping]\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model file\n",
    "\n",
    "model.save('rnn_imdb.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
